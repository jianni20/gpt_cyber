{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threat Intelligence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a cybersecurity SOC analyst with more than 25 years of experience.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    options = {\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    # Make the API call\n",
    "\n",
    "    client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "    response = client.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages,\n",
    "        options=options,\n",
    "        stream=False            \n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_threat_data(file_path):\n",
    "    # Read the raw threat data from the provided file\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_data = file.read()\n",
    "    # Query LLM to identify and categorize potential threats\n",
    "    identified_threats = call_llm(f\"Analyze the following threat data and identify potential threats: {raw_data}\")\n",
    "    # Extract IoCs from the threat data\n",
    "    extracted_iocs = call_llm(f\"Extract all indicators of compromise (IoCs) from the following threat data: {raw_data}\")\n",
    "    # Obtain a detailed context or narrative behind the identified threats\n",
    "    threat_context = call_llm(f\"Provide a detailed context or narrative behind the identified threats in this data: {raw_data}\")\n",
    "    # Print the results\n",
    "    print(\"Identified Threats:\", identified_threats)\n",
    "    print(\"\\nExtracted IoCs:\", extracted_iocs)\n",
    "    print(\"\\nThreat Context:\", threat_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = input(\"Enter the path to the raw threat data .txt file: \")\n",
    "    analyze_threat_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from ollama import Client\n",
    "import os\n",
    "import socket\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a cybersecurity SOC analyst with more than 25 years of experience.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    options = {\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    # Make the API call\n",
    "\n",
    "    client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "    response = client.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages,\n",
    "        options=options,\n",
    "        stream=False            \n",
    "    )\n",
    "    return response['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_syslog():\n",
    "    UDP_IP = \"127.0.0.1\"\n",
    "    UDP_PORT = 5140\n",
    "\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.bind((UDP_IP, UDP_PORT))\n",
    "\n",
    "    while True:\n",
    "        data, addr = sock.recvfrom(1024)\n",
    "        log_entry = data.decode('utf-8')\n",
    "        analysis_result = call_llm(f\"Analyze the following log entry for potential threats: {log_entry} \\n\\nIf you believe there may be suspicious activity, start your response with 'Suspicious Activity: ' and then your analysis. Provide nothing else. If you believe the behavior is normal, start your response with 'Normal Activity: ' and then your analysis. Provide nothing else.\")\n",
    "\n",
    "        if \"Suspicious Activity\" in analysis_result:\n",
    "            print(f\"{analysis_result}\\n\\n\")\n",
    "\n",
    "        await asyncio.sleep(0.1)  # A small delay to allow other tasks to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handler(FileSystemEventHandler):\n",
    "    def process(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        elif event.event_type == 'created':\n",
    "            print(f\"Received file: {event.src_path}\")\n",
    "            with open(event.src_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    analysis_result = call_llm(f\"Analyze the following log entry for potential threats: {line.strip()} \\n\\nIf you believe there may be suspicious activity, start your response with 'Suspicious Activity: ' and then your analysis. Provide nothing else.\")\n",
    "        if \"Suspicious Activity\" in analysis_result:\n",
    "            print(f\"Alert: {analysis_result}\")\n",
    "    def on_created(self, event):\n",
    "        self.process(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Watcher:\n",
    "    DIRECTORY_TO_WATCH = \"/tmp/logs\"\n",
    "    def __init__(self):\n",
    "        self.observer = Observer()\n",
    "    def run(self):\n",
    "        event_handler = Handler()\n",
    "        self.observer.schedule(event_handler,\n",
    "          self.DIRECTORY_TO_WATCH, recursive=False)\n",
    "        self.observer.start()\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except:\n",
    "            self.observer.stop()\n",
    "            print(\"Observer stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # The syslog watcher does not work in a Jupyter notebook as implemented\n",
    "    #asyncio.run(handle_syslog()) \n",
    "    await handle_syslog()\n",
    "    w = Watcher()\n",
    "    w.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a cybersecurity SOC analyst with more than 25 years of experience.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    options = {\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    # Make the API call\n",
    "\n",
    "    client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "    response = client.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages,\n",
    "        options=options,\n",
    "        stream=False            \n",
    "    )\n",
    "    return response['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a command and return its output\n",
    "def run_command(command):\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True)\n",
    "    return result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "# Gather data from key locations\n",
    "# registry_data = run_command('reg query HKLM /s')  # This produces MASSIVE data. Replace with specific registry keys if needed\n",
    "# print(registry_data)\n",
    "process_data = run_command('tasklist /v')\n",
    "print(process_data)\n",
    "network_data = run_command('netstat -an')\n",
    "print(network_data)\n",
    "scheduled_tasks = run_command('schtasks /query /fo LIST')\n",
    "print(scheduled_tasks)\n",
    "security_logs = run_command('wevtutil qe Security /c:10 /rd:true /f:text')  # Last 10 security events. Adjust as needed\n",
    "print(security_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nix\n",
    "# Gather data from key locations\n",
    "process_data = run_command('ps aux')\n",
    "print(process_data)\n",
    "network_data = run_command('ss -tlnp')\n",
    "print(network_data)\n",
    "scheduled_tasks = run_command('crontab -l')\n",
    "print(scheduled_tasks)\n",
    "security_logs = run_command('tail -n 10 /var/log/syslog')  # Last 10 syslog events. Adjust as needed\n",
    "print(security_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the gathered data using ChatGPT\n",
    "analysis_result = call_llm(f\"Analyze the following system data for signs of APTs:\\nProcess Data:\\n{process_data}\\n\\nNetwork Data:\\n{network_data}\\n\\nScheduled Tasks:\\n{scheduled_tasks}\\n\\nSecurity Logs:\\n{security_logs}\") # Add Registry Data:\\n{#registry_data}\\n\\n if used\n",
    "\n",
    "# Display the analysis result\n",
    "print(f\"Analysis Result:\\n{analysis_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "from scapy.all import rdpcap, IP, TCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a cybersecurity SOC analyst with more than 25 years of experience.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    options = {\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    # Make the API call\n",
    "\n",
    "    client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "    response = client.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages,\n",
    "        options=options,\n",
    "        stream=False            \n",
    "    )\n",
    "    return response['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PCAP file\n",
    "packets = rdpcap('example.pcap')\n",
    "\n",
    "# Summarize the traffic (simplified example)\n",
    "ip_summary = {}\n",
    "port_summary = {}\n",
    "protocol_summary = {}\n",
    "for packet in packets:\n",
    "    if packet.haslayer(IP):\n",
    "        ip_src = packet[IP].src\n",
    "        ip_dst = packet[IP].dst\n",
    "        ip_summary[f\"{ip_src} to {ip_dst}\"] = ip_summary.get(f\"{ip_src} to {ip_dst}\", 0) + 1\n",
    "\n",
    "    if packet.haslayer(TCP):\n",
    "        port_summary[packet[TCP].sport] = port_summary.get(packet[TCP].sport, 0) + 1\n",
    "\n",
    "    if packet.haslayer(IP):\n",
    "        protocol_summary[packet[IP].proto] = protocol_summary.get(packet[IP].proto, 0) + 1\n",
    "\n",
    "# Create summary strings\n",
    "ip_summary_str = \"\\n\".join(f\"{k}: {v} packets\" for k, v in ip_summary.items())\n",
    "port_summary_str = \"\\n\".join(f\"Port {k}: {v} packets\" for k, v in port_summary.items())\n",
    "protocol_summary_str = \"\\n\".join(f\"Protocol {k}: {v} packets\" for k, v in protocol_summary.items())\n",
    "\n",
    "# Combine summaries\n",
    "total_summary = f\"IP Summary:\\n{ip_summary_str}\\n\\nPort Summary:\\n{port_summary_str}\\n\\nProtocol Summary:\\n{protocol_summary_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ip_summary_str)\n",
    "print(port_summary_str)\n",
    "print(protocol_summary_str)\n",
    "print(total_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze using LLM\n",
    "analysis_result = call_llm(f\"Analyze the following summarized network traffic for anomalies or potential threats:\\n{total_summary}\")\n",
    "\n",
    "# Print the analysis result\n",
    "print(f\"Analysis Result:\\n{analysis_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-cyber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
