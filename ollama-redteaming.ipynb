{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Red Team Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mitreattack.stix20 import MitreAttackData\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "mitre_attack_data = MitreAttackData(\"enterprise-attack.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords from the description\n",
    "def extract_keywords_from_description(description):\n",
    "    # Define the merged prompt\n",
    "    prompt = (f\"Given the cybersecurity scenario description: '{description}', identify and list the key terms, \"\n",
    "              \"techniques, or technologies relevant to MITRE ATT&CK. Extract TTPs from the scenario. \"\n",
    "              \"If the description is too basic, expand upon it with additional details, applicable campaign, \"\n",
    "              \"or attack types based on dataset knowledge. Then, extract the TTPs from the revised description.\")\n",
    "    # Set up the messages for the Ollama API\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a cybersecurity professional with more than 25 years of experience.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    # Set the request options\n",
    "    options = {\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    # Make the API call\n",
    "    try:\n",
    "        client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "        response = client.chat(\n",
    "            model=\"llama3\",\n",
    "            messages=messages,\n",
    "            options=options,\n",
    "            stream=False            \n",
    "        )\n",
    "        response_content = response['message']['content'].strip()\n",
    "        keywords = response_content.split(', ')\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while connecting to the OpenAI API:\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive scenario\n",
    "def generate_ttp_chain(match):\n",
    "    # Create a prompt for GPT-3 to generate a TTP chain for the provided match\n",
    "    prompt = (f\"Given the MITRE ATT&CK technique '{match['name']}' and its description '{match['description']}', \"\n",
    "              \"generate an example scenario and TTP chain demonstrating its use.\")\n",
    "    # Set up the messages for the OpenAI API\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a cybersecurity professional with expertise in MITRE ATT&CK techniques.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    # Set the request options\n",
    "    options = {\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    # Make the API call\n",
    "    try:\n",
    "        client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "        response = client.chat(\n",
    "            model=\"llama3\",\n",
    "            messages=messages,\n",
    "            options=options,\n",
    "            stream=False            \n",
    "        )\n",
    "        response_content = response['message']['content'].strip()\n",
    "        return response_content\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while generating the TTP chain:\", e)\n",
    "        return \"Unable to generate TTP chain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the ATT&Ck dataset for extracted keywords\n",
    "def score_matches(matches, keywords):\n",
    "    scores = []\n",
    "    for match in matches:\n",
    "        score = sum([keyword in match['name'] for keyword in keywords]) + \\\n",
    "                sum([keyword in match['description'] for keyword in keywords])\n",
    "        scores.append((match, score))\n",
    "    return scores\n",
    "def search_dataset_for_matches(keywords):\n",
    "    matches = []\n",
    "    for item in mitre_attack_data.get_techniques():\n",
    "        if any(keyword in item['name'] for keyword in keywords):\n",
    "            matches.append(item)\n",
    "        elif 'description' in item and any(keyword in item['description'] for keyword in keywords):\n",
    "            matches.append(item)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description = input(\"Enter your scenario description: \")\n",
    "description = \"The scenario is this: an electic utility is being attacked by APT29.\"\n",
    "keywords = extract_keywords_from_description(description)\n",
    "matches = search_dataset_for_matches(keywords)\n",
    "scored_matches = score_matches(matches, keywords)\n",
    "# Sort by score in descending order and take the top 3\n",
    "top_matches = sorted(scored_matches, key=lambda x: x[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing top 3 matches from the MITRE ATT&CK dataset:\")\n",
    "with open(\"ollama_red_team_scenarios.txt\", \"w\") as file:\n",
    "    for match, score in top_matches:\n",
    "        file.write(\"Name:\" + match['name'] +\"\\n\")\n",
    "        file.write(\"Summary:\" + match['description'] + \"\\n\")\n",
    "        ttp_chain = generate_ttp_chain(match)\n",
    "        file.write(\"Example Scenario and TTP Chain:\" + ttp_chain + \"\\n\")\n",
    "        file.write(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Dork Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "CSE_ID = os.getenv(\"CSE_ID\")\n",
    "SEARCH_URL = \"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = (\"You are a cybersecurity professional specializing in red team tactics.\" \n",
    "          \"I am a cybersecurity professional and I have a scenario where I need to find exposed documents on a my own domain.\" \n",
    "          \"Please provide a list of example Google dorks that I can use to discover such vulnerabilities\" \n",
    "          \"as part of an authorized exercise on my own authorized domain.\"\n",
    "          \"At the end of the reponse, include the dorks in a Python list\"          \n",
    "          )\n",
    "client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "response = client.chat(\n",
    "    model='llama3',\n",
    "    messages=[{'role': 'user', 'content': prompt}],\n",
    "    stream=False\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dorks = [\n",
    "    \"site:{} \\\"intranet\\\" -filetype:pdf\".format(\"yourdomain.com\"),\n",
    "    \"site:{} \\\"confidential\\\" OR \\\"proprietary\\\" -filetype:pdf\".format(\"yourdomain.com\"),\n",
    "    \"site:{} \\\"news\\\" \\\"archive\\\" -filetype:pdf\".format(\"yourdomain.com\"),\n",
    "    \"site:{} \\\"HR\\\" OR \\\"finance\\\" OR \\\"security\\\" -inurl:www\".format(\"yourdomain.com\")\n",
    "]\n",
    "dorks_modified = [dork.replace(\"yourdomain.com\", \"fpl.com\") for dork in dorks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(query):\n",
    "    \"\"\"Fetch the Google search results.\"\"\"\n",
    "    response = requests.get(SEARCH_URL.format(query=query, api_key=GOOGLE_API_KEY, cse_id=CSE_ID))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ollama_dork_results.txt\", \"a\") as outfile:\n",
    "    for dork in dorks_modified:\n",
    "        print(f\"Running dork: {dork}\")\n",
    "        results = get_search_results(dork)\n",
    "        if 'items' in results:\n",
    "            for item in results['items']:\n",
    "                print(item['title'])\n",
    "                print(item['link'])\n",
    "                outfile.write(item['title'] + \"\\n\")\n",
    "                outfile.write(item['link'] + \"\\n\")\n",
    "                outfile.write(\"-\" * 50 + \"\\n\")\n",
    "        else:\n",
    "            print(\"No results found or reached API limit!\")\n",
    "        # To not hit the rate limit, introduce a delay between requests\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-powered Terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While it works, this section is better off in a script file than a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath): #Open and read a file\n",
    "    with open(filepath, 'r', encoding='UTF-8') as infile:\n",
    "        return infile.read()\n",
    "def save_file(filepath, content): #Create a new file or overwrite an existing one.\n",
    "    with open(filepath, 'w', encoding='UTF-8') as outfile:\n",
    "        outfile.write(content)\n",
    "def append_file(filepath, content): #Create a new file or append an existing one.\n",
    "    with open(filepath, 'a', encoding='UTF-8') as outfile:\n",
    "        outfile.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3(prompt):\n",
    "    try:\n",
    "        client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "        response = client.chat(\n",
    "            model='llama3',\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        text = response['message']['content'].strip().replace(\"`\",\"\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError communicating with the API.\")\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"\\nRetrying...\")\n",
    "        return llama3(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    request = input(\"\\nEnter request: \")\n",
    "    if not request:\n",
    "        break\n",
    "    if request == \"quit\":\n",
    "        break\n",
    "    prompt = open_file(\"prompt_linux_cli.txt\").replace('{INPUT}', request)\n",
    "    command = llama3(prompt)\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, bufsize=1, universal_newlines=True)\n",
    "    print(\"\\n\" + command + \"\\n\")\n",
    "    with process:\n",
    "        for line in process.stdout:\n",
    "            print(line, end='', flush=True)\n",
    "    exit_code = process.wait()\n",
    "    append_file(\"command-log.txt\", \"Request: \" + request + \"\\nCommand: \" + command + \"\\n\\n\") #Write the request and GPT generated command to a log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-cyber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
