{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incident Response Playbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_incident_response_playbook(threat_type, environment_details):\n",
    "    \"\"\"\n",
    "    Generate an incident response playbook based on the provided threat type and environment details.\n",
    "    \"\"\"\n",
    "    # Create the messages for the OpenAI API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant helping to create an incident response playbook.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Create a detailed incident response playbook for handling a '{threat_type}' threat affecting the following environment: {environment_details}.\"}\n",
    "    ]\n",
    "\n",
    "    # Make the API call\n",
    "    try:\n",
    "        options = {\n",
    "        \"temperature\": 0.7\n",
    "        }\n",
    "        client = Client(host=os.getenv(\"OLLAMA_CHAT\"))\n",
    "        response = client.chat(\n",
    "        model=\"llama3\",\n",
    "        messages=messages,\n",
    "        options=options,\n",
    "        stream=False            \n",
    "    )\n",
    "        return response['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input from the user\n",
    "threat_type = input(\"Enter the threat type: \")\n",
    "environment_details = input(\"Enter environment details: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the playbook\n",
    "playbook = generate_incident_response_playbook(threat_type, environment_details)\n",
    "\n",
    "# Print the generated playbook\n",
    "if playbook:\n",
    "    print(\"\\nGenerated Incident Response Playbook:\")\n",
    "    print(playbook)\n",
    "else:\n",
    "    print(\"Failed to generate the playbook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "import os\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "ollama = Ollama(\n",
    "    base_url='http://192.168.10.80:11434',\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "oembed = OllamaEmbeddings(base_url=\"http://192.168.10.80:11434\", model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_log_to_json(raw_log_path):\n",
    "    #Parses a raw log file and converts it into a JSON format.\n",
    "    # Regular expressions to match timestamps and event descriptions in the raw log\n",
    "    timestamp_regex = r'\\[\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\]'\n",
    "    event_regex = r'Event: (.+)'\n",
    "\n",
    "    json_data = []\n",
    "\n",
    "    with open(raw_log_path, 'r') as file:\n",
    "        for line in file:\n",
    "            timestamp_match = re.search(timestamp_regex, line)\n",
    "            event_match = re.search(event_regex, line)\n",
    "\n",
    "            if timestamp_match and event_match:\n",
    "                timestamp = timestamp_match.group().strip('[]')\n",
    "                event_description = event_match.group(1)\n",
    "                json_data.append({\"Timestamp\": timestamp, \"Event\": event_description})\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        embedding = oembed.embed_documents(text)\n",
    "        \n",
    "        embeddings.append(embedding[0])\n",
    "\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    # Creates a FAISS index for a given set of embeddings.\n",
    "    d = embeddings.shape[1]  # Dimensionality of the embeddings\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(embeddings.astype(np.float32))  # FAISS expects float32\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_logs_with_embeddings(log_data):\n",
    "    # Define your templates and compute their embeddings\n",
    "    suspicious_templates = [\"Unauthorized access attempt detected\", \"Multiple failed login attempts\"]\n",
    "    normal_templates = [\"User logged in successfully\", \"System health check completed\"]\n",
    "    suspicious_embeddings = get_embeddings(suspicious_templates)\n",
    "    normal_embeddings = get_embeddings(normal_templates)\n",
    "\n",
    "    # Combine all template embeddings and create a FAISS index\n",
    "    template_embeddings = np.vstack((suspicious_embeddings, normal_embeddings))\n",
    "    index = create_faiss_index(template_embeddings)\n",
    "\n",
    "    # Labels for each template\n",
    "    labels = ['Suspicious'] * len(suspicious_embeddings) + ['Normal'] * len(normal_embeddings)\n",
    "\n",
    "    categorized_events = []\n",
    "\n",
    "    for entry in log_data:\n",
    "        # Fetch the embedding for the current log entry\n",
    "        log_embedding = get_embeddings([entry[\"Event\"]]).astype(np.float32)\n",
    "\n",
    "        # Perform the nearest neighbor search with FAISS\n",
    "        k = 1  # Number of nearest neighbors to find\n",
    "        _, indices = index.search(log_embedding, k)\n",
    "\n",
    "        # Determine the category based on the nearest template\n",
    "        category = labels[indices[0][0]]\n",
    "        categorized_events.append((entry[\"Timestamp\"], entry[\"Event\"], category))\n",
    "\n",
    "    return categorized_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample raw log file path\n",
    "raw_log_file_path = 'sample_log_file.txt'\n",
    "\n",
    "# Parse the raw log file into JSON format\n",
    "log_data = parse_raw_log_to_json(raw_log_file_path)\n",
    "\n",
    "# Analyze the logs\n",
    "categorized_timeline = analyze_logs_with_embeddings(log_data)\n",
    "\n",
    "# Print the categorized timeline\n",
    "for timestamp, event, category in categorized_timeline:\n",
    "    print(f\"{timestamp} - {event} - {category}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-cyber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
